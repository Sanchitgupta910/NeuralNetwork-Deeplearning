{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OGInoMIzkvEI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(128))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtX6essQk2XX",
        "outputId": "8466cc9f-52ec-4e91-b55c-4698a644e12a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          64000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               98816     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8ef33660a0"
      },
      "source": [
        "Built-in RNNs support a number of useful features:\n",
        "\n",
        "- Recurrent dropout, via the `dropout` and `recurrent_dropout` arguments\n",
        "- Ability to process an input sequence in reverse, via the `go_backwards` argument\n",
        "- Loop unrolling (which can lead to a large speedup when processing short sequences on\n",
        "CPU), via the `unroll` argument\n",
        "- ...and more.\n",
        "\n",
        "For more information, see the\n",
        "[RNN API documentation](https://keras.io/api/layers/recurrent_layers/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43aa4e4f344d"
      },
      "source": [
        "## Outputs and states\n",
        "\n",
        "By default, the output of a RNN layer contains a single vector per sample. This vector\n",
        "is the RNN cell output corresponding to the last timestep, containing information\n",
        "about the entire input sequence. The shape of this output is `(batch_size, units)`\n",
        "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
        "\n",
        "A RNN layer can also return the entire sequence of outputs for each sample (one vector\n",
        "per timestep per sample), if you set `return_sequences=True`. The shape of this output\n",
        "is `(batch_size, timesteps, units)`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lstjURKk6Zk",
        "outputId": "ea877ecf-e6cb-485d-e4bd-bd559ab50840"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 256)         247296    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 128)               49280     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 361,866\n",
            "Trainable params: 361,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266812a04bb2"
      },
      "source": [
        "In addition, a RNN layer can return its final internal state(s). The returned states\n",
        "can be used to resume the RNN execution later, or\n",
        "[to initialize another RNN](https://arxiv.org/abs/1409.3215).\n",
        "This setting is commonly used in the\n",
        "encoder-decoder sequence-to-sequence model, where the encoder final state is used as\n",
        "the initial state of the decoder.\n",
        "\n",
        "To configure a RNN layer to return its internal state, set the `return_state` parameter\n",
        "to `True` when creating the layer. Note that `LSTM` has 2 state  tensors, but `GRU`\n",
        "only has one.\n",
        "\n",
        "To configure the initial state of the layer, just call the layer with additional\n",
        "keyword argument `initial_state`.\n",
        "Note that the shape of the state needs to match the unit size of the layer, like in the\n",
        "example below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab = 1000\n",
        "decoder_vocab = 2000\n",
        "\n",
        "encoder_input = layers.Input(shape=(None,))\n",
        "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
        "    encoder_input\n",
        ")\n",
        "\n",
        "# Return states in addition to output\n",
        "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
        "    encoder_embedded\n",
        ")\n",
        "encoder_state = [state_h, state_c]\n",
        "\n",
        "decoder_input = layers.Input(shape=(None,))\n",
        "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
        "    decoder_input\n",
        ")\n",
        "\n",
        "# Pass the 2 states to a new LSTM layer, as initial state\n",
        "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
        "    decoder_embedded, initial_state=encoder_state\n",
        ")\n",
        "output = layers.Dense(10)(decoder_output)\n",
        "\n",
        "model = keras.Model([encoder_input, decoder_input], output)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zw6tDfSlDQW",
        "outputId": "e853a337-8543-4662-dc00-0cdedec5315f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 64)     64000       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 64)     128000      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " encoder (LSTM)                 [(None, 64),         33024       ['embedding_2[0][0]']            \n",
            "                                 (None, 64),                                                      \n",
            "                                 (None, 64)]                                                      \n",
            "                                                                                                  \n",
            " decoder (LSTM)                 (None, 64)           33024       ['embedding_3[0][0]',            \n",
            "                                                                  'encoder[0][1]',                \n",
            "                                                                  'encoder[0][2]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           650         ['decoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 258,698\n",
            "Trainable params: 258,698\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e97a845a372a"
      },
      "source": [
        "## RNN layers and RNN cells\n",
        "\n",
        "In addition to the built-in RNN layers, the RNN API also provides cell-level APIs.\n",
        "Unlike RNN layers, which processes whole batches of input sequences, the RNN cell only\n",
        "processes a single timestep.\n",
        "\n",
        "The cell is the inside of the `for` loop of a RNN layer. Wrapping a cell inside a\n",
        "`keras.layers.RNN` layer gives you a layer capable of processing batches of\n",
        "sequences, e.g. `RNN(LSTMCell(10))`.\n",
        "\n",
        "Mathematically, `RNN(LSTMCell(10))` produces the same result as `LSTM(10)`. In fact,\n",
        "the implementation of this layer in TF v1.x was just creating the corresponding RNN\n",
        "cell and wrapping it in a RNN layer.  However using the built-in `GRU` and `LSTM`\n",
        "layers enable the use of CuDNN and you may see better performance.\n",
        "\n",
        "There are three built-in RNN cells, each of them corresponding to the matching RNN\n",
        "layer.\n",
        "\n",
        "- `keras.layers.SimpleRNNCell` corresponds to the `SimpleRNN` layer.\n",
        "\n",
        "- `keras.layers.GRUCell` corresponds to the `GRU` layer.\n",
        "\n",
        "- `keras.layers.LSTMCell` corresponds to the `LSTM` layer.\n",
        "\n",
        "The cell abstraction, together with the generic `keras.layers.RNN` class, make it\n",
        "very easy to implement custom RNN architectures for your research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b3b721d500"
      },
      "source": [
        "## Cross-batch statefulness\n",
        "\n",
        "When processing very long sequences (possibly infinite), you may want to use the\n",
        "pattern of **cross-batch statefulness**.\n",
        "\n",
        "Normally, the internal state of a RNN layer is reset every time it sees a new batch\n",
        "(i.e. every sample seen by the layer is assumed to be independent of the past). The\n",
        "layer will only maintain a state while processing a given sample.\n",
        "\n",
        "If you have very long sequences though, it is useful to break them into shorter\n",
        "sequences, and to feed these shorter sequences sequentially into a RNN layer without\n",
        "resetting the layer's state. That way, the layer can retain information about the\n",
        "entirety of the sequence, even though it's only seeing one sub-sequence at a time.\n",
        "\n",
        "You can do this by setting `stateful=True` in the constructor.\n",
        "\n",
        "If you have a sequence `s = [t0, t1, ... t1546, t1547]`, you would split it into e.g.\n",
        "\n",
        "```\n",
        "s1 = [t0, t1, ... t100]\n",
        "s2 = [t101, ... t201]\n",
        "...\n",
        "s16 = [t1501, ... t1547]\n",
        "```\n",
        "\n",
        "Then you would process it via:\n",
        "\n",
        "```python\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "for s in sub_sequences:\n",
        "  output = lstm_layer(s)\n",
        "```\n",
        "\n",
        "When you want to clear the state, you  can use `layer.reset_states()`.\n",
        "\n",
        "\n",
        "> Note: In this setup, sample `i` in a given batch is assumed to be the continuation of\n",
        "sample `i` in the previous batch. This means that all batches should contain the same\n",
        "number of samples (batch size). E.g. if a batch contains `[sequence_A_from_t0_to_t100,\n",
        " sequence_B_from_t0_to_t100]`, the next batch should contain\n",
        "`[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here is a complete example:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "output = lstm_layer(paragraph3)\n",
        "\n",
        "# reset_states() will reset the cached state to the original initial_state.\n",
        "# If no initial_state was provided, zero-states will be used by default.\n",
        "lstm_layer.reset_states()\n"
      ],
      "metadata": {
        "id": "Bcx2OKEblKgY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7c316b19a1"
      },
      "source": [
        "### RNN State Reuse\n",
        "<a id=\"rnn_state_reuse\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cb7a8ac464a"
      },
      "source": [
        "The recorded states of the RNN layer are not included in the `layer.weights()`. If you\n",
        "would like to reuse the state from a RNN layer, you can retrieve the states value by\n",
        "`layer.states` and use it as the\n",
        "initial state for a new layer via the Keras functional API like `new_layer(inputs,\n",
        "initial_state=layer.states)`, or model subclassing.\n",
        "\n",
        "Please also note that sequential model might not be used in this case since it only\n",
        "supports layers with single input and output, the extra input of initial state makes\n",
        "it impossible to use here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
        "\n",
        "lstm_layer = layers.LSTM(64, stateful=True)\n",
        "output = lstm_layer(paragraph1)\n",
        "output = lstm_layer(paragraph2)\n",
        "\n",
        "existing_state = lstm_layer.states\n",
        "\n",
        "new_lstm_layer = layers.LSTM(64)\n",
        "new_output = new_lstm_layer(paragraph3, initial_state=existing_state)\n"
      ],
      "metadata": {
        "id": "ECzqfwcilVuo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66c1d7f1ccba"
      },
      "source": [
        "## Bidirectional RNNs\n",
        "\n",
        "For sequences other than time series (e.g. text), it is often the case that a RNN model\n",
        "can perform better if it not only processes sequence from start to end, but also\n",
        "backwards. For example, to predict the next word in a sentence, it is often useful to\n",
        "have the context around the word, not only just the words that come before it.\n",
        "\n",
        "Keras provides an easy API for you to build such bidirectional RNNs: the\n",
        "`keras.layers.Bidirectional` wrapper."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "model.add(\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
        ")\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Am8RIkliEu",
        "outputId": "0e1c1c9e-052e-400c-ec33-68115147efea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirectiona  (None, 5, 128)           38400     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,266\n",
            "Trainable params: 80,266\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dab57c97a566"
      },
      "source": [
        "Under the hood, `Bidirectional` will copy the RNN layer passed in, and flip the\n",
        "`go_backwards` field of the newly copied layer, so that it will process the inputs in\n",
        "reverse order.\n",
        "\n",
        "The output of the `Bidirectional` RNN will be, by default, the concatenation of the forward layer\n",
        "output and the backward layer output. If you need a different merging behavior, e.g.\n",
        "concatenation, change the `merge_mode` parameter in the `Bidirectional` wrapper\n",
        "constructor. For more details about `Bidirectional`, please check\n",
        "[the API docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional/)."
      ]
    }
  ]
}